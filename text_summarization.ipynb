{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chart_studio\n",
    "# !pip install pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pydot\n",
    "import graphviz\n",
    "\n",
    "import os\n",
    "# # print(os.listdir(\"../input\"))\n",
    "# import chart_studio.plotly as py\n",
    "# import chart_studio.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import re\n",
    "import string\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, sys, tarfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input,Dense, Activation, concatenate, Embedding, Flatten, Bidirectional, Concatenate, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kindle_reviews = pd.read_csv('kindle_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting the dataframe display\n",
    "\n",
    "pd.set_option('display.max_info_columns',1000)\n",
    "pd.set_option('display.max_colwidth',5000)\n",
    "kindle_reviews.drop(columns = ['asin', 'helpful', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime'],\n",
    "                    axis=1, inplace=True)\n",
    "kindle_reviews.columns = ['index','review', 'summary']\n",
    "kindle_reviews.set_index(keys = 'index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_VOCAB = 10000                           # most frequent 15K words form the vocab \n",
    "MAX_SEQUENCE_REVIEW_LENGTH = 22             \n",
    "MAX_SEQUENCE_SUMMARY_LENGTH = 9\n",
    "EMBEDDING_DIM = 200                         \n",
    "EMBEDDING_FILE_PATH = \"glove.6B.\" + str(EMBEDDING_DIM) + \"d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text,remove_stopwords = True, max_len = 20):\n",
    "    '''\n",
    "    Given a text this function removes the punctuations, selected stopwords(because not, none convey some meaning and\n",
    "    removing these stop words changes the meaning of the sentence.) and returns the length of the remaining text string\n",
    "    '''\n",
    "    refined_stop_words = {}\n",
    "    if(remove_stopwords == True):\n",
    "        refined_stop_words = stop_words.ENGLISH_STOP_WORDS-{ \"not\",\"none\",\"nothing\",\"nowhere\",\"never\",\n",
    "                                                        \"cannot\",\"cant\",\"couldnt\",\"except\",\"hasnt\",\n",
    "                                                        \"neither\",\"no\",\"nobody\",\"nor\",\"without\"\n",
    "                                                           }\n",
    "    try:\n",
    "        #convert to lower case and strip regex\n",
    "        new_text = []\n",
    "        text = text.lower()\n",
    "        count = 0\n",
    "        for word in text.split():\n",
    "            if word in refined_stop_words:\n",
    "                continue\n",
    "            count += 1\n",
    "            if word in contractions: \n",
    "                new_text = new_text + [contractions[word]]\n",
    "            else: \n",
    "                new_text = new_text + [word]\n",
    "        new_text = new_text[0:max_len] if count>max_len else new_text\n",
    "        text = ' '.join(new_text)\n",
    "        regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
    "        text = regex.sub(\" \", text)\n",
    "        text = re.sub('\\s+', ' ', text).strip()\n",
    "        text = '<start> ' + text + ' <end>'\n",
    "        return text\n",
    "\n",
    "\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; enjoy vintage books movies enjoyed reading book plot unusual do not think killing self defense leaving scene body without notifying police hitting &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; nice vintage story &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; book reissue old one author born 1910 it is era of say nero wolfe introduction quite interesting explaining author he is forgotten &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; different &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; fairly interesting read old style terminology i glad read story does not coarse crasslanguage read fun relaxation i like free ebooksbecause check writer &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; oldie &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; i would never read amy brewster mysteries one really hooked now &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; really liked it &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; like period pieces clothing lingo enjoy mystery author guessing 2 3 way through &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; period mystery &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                       review  \\\n",
       "index                                                                                                                                                                           \n",
       "0            <start> enjoy vintage books movies enjoyed reading book plot unusual do not think killing self defense leaving scene body without notifying police hitting <end>   \n",
       "1                            <start> book reissue old one author born 1910 it is era of say nero wolfe introduction quite interesting explaining author he is forgotten <end>   \n",
       "2      <start> fairly interesting read old style terminology i glad read story does not coarse crasslanguage read fun relaxation i like free ebooksbecause check writer <end>   \n",
       "3                                                                                               <start> i would never read amy brewster mysteries one really hooked now <end>   \n",
       "4                                                                               <start> like period pieces clothing lingo enjoy mystery author guessing 2 3 way through <end>   \n",
       "\n",
       "                                summary  \n",
       "index                                    \n",
       "0      <start> nice vintage story <end>  \n",
       "1               <start> different <end>  \n",
       "2                   <start> oldie <end>  \n",
       "3         <start> really liked it <end>  \n",
       "4          <start> period mystery <end>  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kindle_reviews['summary'] = kindle_reviews.summary.apply(lambda x: clean_text(x, True, MAX_SEQUENCE_SUMMARY_LENGTH-2))\n",
    "kindle_reviews['review'] = kindle_reviews.review.apply(lambda x: clean_text(x, True, MAX_SEQUENCE_REVIEW_LENGTH-2))\n",
    "\n",
    "kindle_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982619, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kindle_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = Tokenizer(num_words = RNN_VOCAB, oov_token='OOV', filters = '')\n",
    "tokenize.fit_on_texts(np.hstack([kindle_reviews['summary'],kindle_reviews['review']]))\n",
    "kindle_reviews['sequence_summary'] = tokenize.texts_to_sequences(kindle_reviews['summary'])\n",
    "kindle_reviews['sequence_review'] = tokenize.texts_to_sequences(kindle_reviews['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#craeting dataset for the model input output\n",
    "dataset = {}\n",
    "dataset['decoder_input'] = pad_sequences(kindle_reviews.sequence_summary, maxlen = MAX_SEQUENCE_SUMMARY_LENGTH, padding='post')\n",
    "kindle_reviews['sequence_summary'] = kindle_reviews.sequence_summary.apply(lambda x: x[1:])\n",
    "dataset['decoder_output'] = pad_sequences(kindle_reviews.sequence_summary, maxlen = MAX_SEQUENCE_SUMMARY_LENGTH-1, padding='post')\n",
    "dataset['encoder_input'] = pad_sequences(kindle_reviews.sequence_review, maxlen = MAX_SEQUENCE_REVIEW_LENGTH, padding='pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((982619, 8), (982619, 9), (982619, 22))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['decoder_output'].shape, dataset['decoder_input'].shape, dataset['encoder_input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6739,   20, 1871,   26,   22,    4,   75,  943,   44,    6,   52,\n",
       "        1647,  364, 5845, 1108,  576,  618,  203,    1, 1246, 3933,    3],\n",
       "       [   1,  152,   59,   23, 1337,    1,   12,   21, 1899,  736,   51,\n",
       "        9802, 4205,  693,  178,   40, 1991,   23,  298,   21, 2410,    3]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['encoder_input'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kindle_val_reviews = kindle_reviews.loc[1000:1005,:].copy()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    \"\"\"\n",
    "    When a corpus is passed, remove the words which are not in the global vocab(glove) and use most frequent vocab_size\n",
    "    number of words. \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def readEmbeddings(self, filePath):\n",
    "        \"\"\"\n",
    "        Given a filepath of word embeddings creates and returns a dictionary of word, embedding values\n",
    "        \"\"\"\n",
    "        # Create a dictionary for storing all {word, embedding values}\n",
    "        wordToEmbeddingDict = {}\n",
    "        # open the file as read only\n",
    "        file = open(filePath, encoding='utf-8')\n",
    "        # read all text\n",
    "        for line in file:\n",
    "            lineValue = line.split()\n",
    "            word = lineValue[0]\n",
    "            embedding = np.asarray(lineValue[1:],dtype = 'float32')\n",
    "            wordToEmbeddingDict[word] = embedding\n",
    "        # close the file\n",
    "        file.close()\n",
    "        return wordToEmbeddingDict\n",
    "    \n",
    "    def indexToEmbedding(self, wordToIndexDict, wordToEmbeddingDict):\n",
    "        indexToEmbeddingMatrix = np.zeros((self.vocab_size, self.embedding_dim))\n",
    "        for word, index in wordToIndexDict.items():\n",
    "            if index >= self.vocab_size:\n",
    "                break\n",
    "            if word in wordToEmbeddingDict.keys():\n",
    "                indexToEmbeddingMatrix[index] = wordToEmbeddingDict[word]\n",
    "            else:\n",
    "                indexToEmbeddingMatrix[index] = np.array(np.random.uniform(-1.0, 1.0, self.embedding_dim))\n",
    "\n",
    "        return indexToEmbeddingMatrix\n",
    "    \n",
    "    def indexToWord(self, wordToIndexDict):\n",
    "        return {index: word for word, index in wordToIndexDict.items()}\n",
    "\n",
    "embeddings = Embeddings(EMBEDDING_DIM, RNN_VOCAB)\n",
    "wordToEmbeddingDict = embeddings.readEmbeddings(EMBEDDING_FILE_PATH)\n",
    "\n",
    "indexToEmbeddingMatrix = embeddings.indexToEmbedding(tokenize.word_index, wordToEmbeddingDict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexToWordDict = embeddings.indexToWord(tokenize.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2096\n",
    "NUM_EPOCHS = 3\n",
    "STEPS_PER_EPOCH = 150\n",
    "LATENT_DIM = 512                           # Dimensions of LSTM output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_batch_data(dataset, start, end):\n",
    "    # Decoder output will be one hot encoded values \n",
    "    # dimensions of the decoder output will be (number of samples * summary length * vocab size)\n",
    "    assert start < end\n",
    "    assert end <= dataset['encoder_input'].shape[0]\n",
    "    encoder_batch_input = dataset['encoder_input'][start:end]\n",
    "    decoder_batch_input = dataset['decoder_input'][start:end]\n",
    "    decoder_batch_output = np.zeros(((end-start), MAX_SEQUENCE_SUMMARY_LENGTH, RNN_VOCAB), dtype = 'float16')\n",
    "    for k, row in enumerate(dataset['decoder_output'][start:end]):\n",
    "        for i,value in enumerate(row):\n",
    "            if value!=0:\n",
    "                decoder_batch_output[k, i, value] = 1\n",
    "    return encoder_batch_input, decoder_batch_input, decoder_batch_output\n",
    "\n",
    "#This generate method loops indefinitely on our dataset to create training batches\n",
    "def generate_batch_data(dataset):\n",
    "    size = dataset['encoder_input'].shape[0]\n",
    "    while True:\n",
    "        start = 0\n",
    "        end = start+BATCH_SIZE\n",
    "        while True:\n",
    "            # create numpy arrays of input data\n",
    "            # and labels, from each line in the file\n",
    "            if start>=size:\n",
    "                break\n",
    "            encoder_batch_input, encoder_batch_output, decoder_batch_output = get_batch_data(dataset, start, end)\n",
    "            start = end\n",
    "            end = np.min([end+BATCH_SIZE, size])\n",
    "            yield ({'review': encoder_batch_input, \n",
    "                    'summary': encoder_batch_output},\n",
    "                   {'decoder_dense_layer': decoder_batch_output})\n",
    "\n",
    "review_input_layer = Input(batch_shape=(BATCH_SIZE, MAX_SEQUENCE_REVIEW_LENGTH, ), name = 'review')\n",
    "embedding_encoder_layer = Embedding(input_length = MAX_SEQUENCE_REVIEW_LENGTH,\n",
    "                          input_dim = RNN_VOCAB,\n",
    "                          output_dim = EMBEDDING_DIM,\n",
    "                          weights=[indexToEmbeddingMatrix],\n",
    "                          trainable = False,\n",
    "                          name = 'embedding_encoder',\n",
    "                          )\n",
    "embedding_review_output = embedding_encoder_layer(review_input_layer)\n",
    "encoder_lstm_layer = Bidirectional(LSTM(LATENT_DIM, return_state=True, name = 'lstm_encoder', stateful = True), merge_mode = 'concat')\n",
    "_, forward_h, forward_c, backward_h, backward_c = encoder_lstm_layer(embedding_review_output)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "summary_input_layer = Input(batch_shape=(BATCH_SIZE ,MAX_SEQUENCE_SUMMARY_LENGTH, ), name = 'summary')\n",
    "embedding_decoder_layer = Embedding(#input_length = MAX_SEQUENCE_SUMMARY_LENGTH,\n",
    "                          input_dim = RNN_VOCAB,\n",
    "                          output_dim = EMBEDDING_DIM,\n",
    "                          weights=[indexToEmbeddingMatrix],\n",
    "                          trainable=False,\n",
    "                          name = 'embedding_decoder',\n",
    "                          )\n",
    "embedding_summary_output = embedding_decoder_layer(summary_input_layer)\n",
    "decoder_lstm_layer = LSTM(2*LATENT_DIM, return_state=True, return_sequences = True, name = 'lstm_decoder', stateful = True)\n",
    "decoder_output, decoder_h, decoder_c = decoder_lstm_layer(embedding_summary_output,\n",
    "                                                   initial_state = encoder_states)\n",
    "\n",
    "decoder_dense_layer = Dense(RNN_VOCAB, activation=\"softmax\", name='decoder_dense_layer')\n",
    "decoder_dense_output =  decoder_dense_layer(decoder_output)\n",
    "\n",
    "model = Model([review_input_layer, summary_input_layer], decoder_dense_output) \n",
    "sgd = optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('summarization_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('summarization_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "# Define inference model\n",
    "encoder_inference_model = Model(review_input_layer, encoder_states)\n",
    "plot_model(encoder_inference_model, to_file='inference_encoder.png', show_shapes=True)\n",
    "\n",
    "# Image(filename='inference_encoder.png') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
